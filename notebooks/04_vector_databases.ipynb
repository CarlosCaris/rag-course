{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4: Retrieval Methods and Vector Databases\n",
    "\n",
    "**Objective**: Build a retrieval system that efficiently searches for relevant document chunks.\n",
    "\n",
    "**Topics**:\n",
    "- Sparse vs. dense retrieval methods\n",
    "- Hybrid search methods (e.g., combining BM25 with dense retrieval)\n",
    "- Overview of vector databases: Milvus, Faiss, Qdrant\n",
    "\n",
    "**Practical Task**: Set up a vector database and implement a retrieval method.\n",
    "\n",
    "**Resources**:\n",
    "- What is a vector database\n",
    "- Choosing a vector database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = (\n",
    "    \"../data/Regulaciones cacao y chocolate 2003.pdf\"\n",
    ")\n",
    "loader = PyPDFLoader(file_path)\n",
    "splitted_doc = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "embedded_document = embedding_model.embed_query(splitted_doc[0].page_content)\n",
    "embedded_document[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(path=\"/tmp/langchain_qdrant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_collection(collection_name=\"demo_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_collection(\n",
    "    collection_name=\"demo_collection\",\n",
    "    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"demo_collection\",\n",
    "    embedding=embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add_documents(splitted_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.scroll(collection_name=\"demo_collection\", limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import RetrievalMode\n",
    "\n",
    "qdrant = QdrantVectorStore.from_documents(\n",
    "    splitted_doc,\n",
    "    embedding=embedding_model,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"my_documents\",\n",
    "    retrieval_mode=RetrievalMode.DENSE,\n",
    ")\n",
    "\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "found_docs = qdrant.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Vector Search\n",
    "\n",
    "To search with only sparse vectors,\n",
    "\n",
    "The retrieval_mode parameter should be set to RetrievalMode.SPARSE.\n",
    "An implementation of the SparseEmbeddings interface using any sparse embeddings provider has to be provided as value to the sparse_embedding parameter.\n",
    "The langchain-qdrant package provides a FastEmbed based implementation out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import FastEmbedSparse, RetrievalMode\n",
    "\n",
    "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\", cache_dir=\"cache\")\n",
    "\n",
    "qdrant = QdrantVectorStore.from_documents(\n",
    "    splitted_doc,\n",
    "    embedding=embedding_model,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"my_documents\",\n",
    "    retrieval_mode=RetrievalMode.SPARSE,\n",
    ")\n",
    "\n",
    "query = \"What is chocolate?\"\n",
    "found_docs = qdrant.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Search\n",
    "\n",
    "To perform a hybrid search using dense and sparse vectors with score fusion,\n",
    "\n",
    "The retrieval_mode parameter should be set to RetrievalMode.HYBRID.\n",
    "A dense embeddings value should be provided to the embedding parameter.\n",
    "An implementation of the SparseEmbeddings interface using any sparse embeddings provider has to be provided as value to the sparse_embedding parameter.\n",
    "Note that if you've added documents with the HYBRID mode, you can switch to any retrieval mode when searching. Since both the dense and sparse vectors are available in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import FastEmbedSparse, RetrievalMode\n",
    "\n",
    "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "\n",
    "qdrant = QdrantVectorStore.from_documents(\n",
    "    splitted_doc,\n",
    "    embedding=embedding_model,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"my_documents\",\n",
    "    retrieval_mode=RetrievalMode.HYBRID,\n",
    ")\n",
    "\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "found_docs = qdrant.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to execute a similarity search and receive the corresponding scores you can run:\n",
    "results = vector_store.similarity_search_with_score(\n",
    "    query=\"Will it be hot tomorrow\", k=1\n",
    ")\n",
    "for doc, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http import models\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    query=\"Who are the best soccer players in the world?\",\n",
    "    k=1,\n",
    "    filter=models.Filter(\n",
    "        should=[\n",
    "            models.FieldCondition(\n",
    "                key=\"page_content\",\n",
    "                match=models.MatchValue(\n",
    "                    value=\"The top 10 soccer players in the world right now.\"\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query by turning into a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 1})\n",
    "retriever.invoke(\"Stealing from the bank is a crime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

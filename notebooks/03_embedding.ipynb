{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: Embedding Models for Retrieval\n",
    "\n",
    "**Objective**: Understand the role of embeddings in representing document chunks for retrieval.\n",
    "\n",
    "**Topics**:\n",
    "- Overview of embedding models: LLM-Embedder, BAAI/bge, etc.\n",
    "- Selecting the right embedding model\n",
    "- Integrating embeddings into the retrieval pipeline\n",
    "\n",
    "**Practical Task**: Implement and test embedding models on the chunked documents.\n",
    "\n",
    "**Resources**:\n",
    "- Choosing and embedding model\n",
    "- How to select an embedding model\n",
    "- [Mastering RAG: How to Select an Embedding Model](https://www.rungalileo.io/blog/mastering-rag-how-to-select-an-embedding-model#:~:text=Embeddings%20encode%20the%20semantics%20of,efficient%20and%20user%20friendly%20experience.)\n",
    "- [Vector Embeddings in RAG Applications](https://wandb.ai/mostafaibrahim17/ml-articles/reports/Vector-Embeddings-in-RAG-Applications--Vmlldzo3OTk1NDA5)\n",
    "- [Vector Embeddings in RAG Applications](https://medium.com/thedeephub/vector-embeddings-in-rag-applications-9ea8043c172b)\n",
    "- [Embeddings leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "\n",
    "file_path = \"../data/Regulaciones cacao y chocolate 2003.pdf\"\n",
    "loader = PDFMinerLoader(file_path)\n",
    "splitted_doc = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Embeddings\n",
    "\n",
    "-Dense embeddings are continuous, low-dimensional vectors where each dimension holds meaningful information. They are typically generated using neural networks and represent data in a compressed form. Dense embeddings are commonly used in deep learning models, capturing semantic similarity between inputs like documents and queries. In RAG, dense embeddings help retrieve documents that are semantically similar to a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_qdrant import FastEmbedSparse, RetrievalMode\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_qdrant import RetrievalMode\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "open_source_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "\n",
    "\n",
    "qdrant = QdrantVectorStore.from_documents(\n",
    "    splitted_doc,\n",
    "    embedding=open_source_embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"my_documents\",\n",
    "    retrieval_mode=RetrievalMode.DENSE,\n",
    ")\n",
    "\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "found_docs = qdrant.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "query_result[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse embeddings\n",
    "\n",
    "- Sparse embeddings are high-dimensional vectors, where most elements are zero, and only a few dimensions hold non-zero values. They are often derived from traditional methods like TF-IDF or BM25, focusing on specific keywords or tokens present in the text. In the context of RAG, sparse embeddings help match documents and queries based on exact term overlap rather than broader semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed.sparse.bm25 import Bm25\n",
    "\n",
    "bm25_embedding_model = Bm25(\"Qdrant/bm25\")\n",
    "bm25_embeddings = list(bm25_embedding_model.passage_embed(splitted_doc[0].page_content))\n",
    "len(bm25_embeddings[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_embeddings_model = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "sparse_embeddings = sparse_embeddings_model.embed_query(\"This is a test document.\")\n",
    "sparse_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Late interaction embeddings\n",
    "\n",
    "- Late interaction embeddings refer to a hybrid approach where sparse and dense embeddings are combined during the retrieval process, but their interactions are delayed until the final ranking or matching phase. This allows the model to first retrieve candidates based on simpler, faster methods and then refine the ranking through more complex, dense representations. In RAG, this helps improve retrieval quality while balancing computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed.late_interaction import LateInteractionTextEmbedding\n",
    "\n",
    "late_interaction_embedding_model = LateInteractionTextEmbedding(\"colbert-ir/colbertv2.0\")\n",
    "late_interaction_embeddings = list(late_interaction_embedding_model.passage_embed(splitted_doc[0].page_content))\n",
    "len(late_interaction_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
